{
    "contents" : "---\ntitle: \"Informe\"\nauthor: \"Leonardo Santella | Simon Saman | Eloy Toro\"\ndate: \"Monday, August 10, 2015\"\noutput: html_document\n---\n\n1. Realice las tareas de pre-procesamiento necesarias para mejorar e identicar las palabras\ncontenidas en cada post.\n\nEn principio cargamos los datos y las librerias necesarias\n\n```{r}\nlibrary(tm)\nlibrary(FactoMineR)\nsetwd(\"~/Data Science/ProyectoICD\")\ndata <- read.csv(\"data.csv\", sep=\";\", row.names=1)\n\n```\n\nPara la limpieza de los datos consideramos las siguientes funciones definidas por nosotros\n\n```{r}\n#Esta funcion elimina caracteres seguidos, repetidos, de manera que hara mas eficiente el estudio.\n#Forma una raiz para los diferentes tipos de risas escritas\n#Forma un solo termino para la palabra video (elimina el plural \"videos\")\nremoveCharacters<- function(x){\n  x<-gsub(\"aa+\",\"a\", x)\n  x<-gsub(\"bb+]\",\"b\", x)\n  x<-gsub(\"(cc)+c+\",\"c\", x)\n  x<-gsub(\"dd+\",\"d\", x)\n  x<-gsub(\"ee+\",\"e\", x)\n  x<-gsub(\"ff+\",\"f\", x)\n  x<-gsub(\"gg+\",\"g\", x)\n  x<-gsub(\"hh+\",\"h\", x)\n  x<-gsub(\"ii+\",\"i\", x)\n  x<-gsub(\"jj+\",\"j\", x)\n  x<-gsub(\"kk+\",\"k\", x)\n  x<-gsub(\"(ll)+l+\",\"l\", x)\n  x<-gsub(\"mm+\",\"m\", x)\n  x<-gsub(\"nn+\",\"n\", x)\n  x<-gsub(\"oo+\",\"o\", x)\n  x<-gsub(\"pp+\",\"p\", x)\n  x<-gsub(\"qq+\",\"q\", x)\n  x<-gsub(\"rr+r+\",\"r\", x)\n  x<-gsub(\"ss+\",\"s\", x)\n  x<-gsub(\"tt+\",\"t\", x)\n  x<-gsub(\"uu+\",\"u\", x)\n  x<-gsub(\"vv+\",\"v\", x)\n  x<-gsub(\"ww+\",\"w\", x)\n  x<-gsub(\"xx+\",\"x\", x)\n  x<-gsub(\"yy+\",\"y\", x)\n  x<-gsub(\"zz+\",\"z\", x)\n  x<-gsub(\"(jaja)+(ja)*\", \"haha\", x)\n  x<-gsub(\"(jeje)+(je)*\", \"haha\", x)\n  x<-gsub(\"(juju)+(ju)*\", \"haha\", x)\n  x<-gsub(\"(jojo)+(jo)*\", \"haha\", x)\n  x<-gsub(\"(hehe)+(he)*\", \"haha\", x)\n  x<-gsub(\"(haha)+(ha)*\", \"haha\", x)\n  x<-gsub(\"[jska]{6}[jska]*\",\"haha\",x)\n  x<-gsub(\"[hsdg]{6}[hdgs]*\",\"haha\",x)\n  x<-gsub(\"(xd)+\",\"haha\", x)\n  x<-gsub(\"(lol)+[lo]*\", \"haha\",x)\n  x<-gsub(\"videos\", \"video\", x)\n  x\n}\n\n#Elimina iconos con codificacion UTF-8\nremoveUTFIcons <- function(x){\n  gsub(\"<[[:alnum:]+[:alnum:]+]+>\",\"\",x)\n}\n\n#Elimina los numeros\nremoveNumbers <- function(x){\n  gsub(\"[[:digit:]]\", \"\", x)\n}\n\n#Elimina los caracteres individuales y los espacios en blanco al inicio del post\nremoveSingles <- function(x){\n x <- gsub(\"^.$\",\"\",x)\n x <- gsub(\"^.[[:space:]]$\",\"\",x)\n x <- gsub(\"^[[:space:]]\", \"\", x)\n x\n}\n\n#Elimina los URL de los datos\nremoveURL <- function(x){\n  x<-gsub(\"http[[:alnum:]]*\", \"\", x)\n  x<-gsub(\"www[[:alnum:]]*\", \"\", x)\n  x\n} \n\n#Inserta una fila en un data frame\ninsertRow <- function(existingDF, newrow, r) {\n  existingDF[seq(r+1,nrow(existingDF)+1),] <- existingDF[seq(r,nrow(existingDF)),]\n  existingDF[r,] <- newrow\n  existingDF\n}\n\n```\n\nAhora procedemos a limpiar el set de datos, sin formar un corpus ya que de esta manera podemos observar de forma mas comoda el comportamiento de las acciones llevadas a cabo\n```{r}\n#Inicio Limpieza de datos\n\n#Eliminamos las filas NA\ndata <- na.omit(data)\n\n#Eliminamos la redundacia de espacios en blanco\ndata$post<-stripWhitespace(as.character(data$post))\n\n#Elimina las filas que solo contienen un caracter no significativo\ndata<- data[!(is.na(data$post) | data$post==\"\" | data$post==\" \" | data$post==\"\\n\" | data$post==\"\\v\") | data$post==\"\\t\" | data$post==\"\\r\" | data$post==\"\\f\",]\n\n#Colocamos todas las letras en minuscula\ndata$post <- tolower(data$post)\n\n#Removemos palabras no significativas para nuestro estudio\nmyStopwords <- c(stopwords(kind=\"es\"), stopwords(kind='en'), 'mierda', 'verga', 'puta', 'fuck')\ndata$post <- removeWords(data$post, myStopwords)\n\n#Eliminamos iconos codificados en UTF-8 (Algunos)\ndata$post <- removeUTFIcons(data$post)\n\n#Eliminamos signos de puntuacion\ndata$post <- removePunctuation(data$post)\n\n#Eliminaos los URL \ndata$post <- removeURL(data$post)\n\n#Eliminamos por 2da vez la redundacia de espacios en blanco\ndata$post<-stripWhitespace(as.character(data$post))\n\n#Eliminamos por 2da vez las filas que contienen catracteres no significativos\ndata<- data[!(is.na(data$post) | data$post==\"\" | data$post==\" \" | data$post==\"\\n\" | data$post==\"\\v\") | data$post==\"\\t\" | data$post==\"\\r\" | data$post==\"\\f\",]\n\n#Eliminamos varios elementos antes mencionados en la definicion de esta funcion\ndata$post <- removeCharacters(data$post)\n\n#Eliminamos los numeros\ndata$post <- removeNumbers(data$post)\n\n#Eliminamos caracteres individuales y espacios en blanco al inicio del post\ndata$post <- removeSingles(data$post)\n\n#Eliminamos por ultima vez la redundancia de espacios en blanco\ndata$post<-stripWhitespace(as.character(data$post))\n\n#Eliminamos las filas con caracteres no signuficativos\ndata<- data[!(is.na(data$post) | data$post==\"\" | data$post==\" \" | data$post==\"\\n\" | data$post==\"\\v\") | data$post==\"\\t\" | data$post==\"\\r\" | data$post==\"\\f\",]\n```\n\n\n2. para cada individuo identifique las palabras que los relacionan con los otros.\n\nPara determinar las palabras que relacionan a los usuarios entre ellos, debemos primero formar ciertas estructuras\n```{r}\n#Armamos 5 data frames, uno por usuario\npostPersona1 <- data[data$id_user==1,]\npostPersona2 <- data[data$id_user==2,]\npostPersona3 <- data[data$id_user==3,]\npostPersona4 <- data[data$id_user==4,]\npostPersona5 <- data[data$id_user==5,]\n\n#Armamos los corpus para luego armar las term-document matrix\ncorpus1 <- Corpus(VectorSource(postPersona1))\ncorpus2 <- Corpus(VectorSource(postPersona2))\ncorpus3 <- Corpus(VectorSource(postPersona3))\ncorpus4 <- Corpus(VectorSource(postPersona4))\ncorpus5 <- Corpus(VectorSource(postPersona5))\n\n#Armamos las matrices respectivas\ntdm1 <- TermDocumentMatrix(corpus1,control=list(wordLengths=c(2,Inf)))\ntdm2 <- TermDocumentMatrix(corpus2,control=list(wordLengths=c(2,Inf)))\ntdm3 <- TermDocumentMatrix(corpus3,control=list(wordLengths=c(2,Inf)))\ntdm4 <- TermDocumentMatrix(corpus4,control=list(wordLengths=c(2,Inf)))\ntdm5 <- TermDocumentMatrix(corpus5,control=list(wordLengths=c(2,Inf)))\n\n#Extraemos los terminos de cada matriz\nterminos1<- as.data.frame(unlist(findFreqTerms(tdm1, lowfreq=1)))\nterminos2<- as.data.frame(unlist(findFreqTerms(tdm2, lowfreq=1)))\nterminos3 <- as.data.frame(unlist(findFreqTerms(tdm3, lowfreq=1)))\nterminos4 <- as.data.frame(unlist(findFreqTerms(tdm4, lowfreq=1)))\nterminos5 <- as.data.frame(unlist(findFreqTerms(tdm5, lowfreq=1)))\n\n#Calculamos la frecuencia de cada termino por usuario\ntermFrequency1 <- rowSums(as.matrix(tdm1))\ntermFrequency2 <- rowSums(as.matrix(tdm2))\ntermFrequency3 <- rowSums(as.matrix(tdm3))\ntermFrequency4 <- rowSums(as.matrix(tdm4))\ntermFrequency5 <- rowSums(as.matrix(tdm5))\n\n#Inicializamos un data frame por usuario\nUsuario1 <- data.frame( Termino=character(3700), frecuencia=numeric(3700))\nUsuario2 <- data.frame( Termino=character(1545), frecuencia=numeric(1545)) \nUsuario3 <- data.frame( Termino=character(2800), frecuencia=numeric(2800)) \nUsuario4 <- data.frame( Termino=character(3222), frecuencia=numeric(3222)) \nUsuario5 <- data.frame( Termino=character(3282), frecuencia=numeric(3282)) \n\n#LLenamos los data frame \nUsuario1$Termino <- terminos1[,1]\nUsuario1$frecuencia <- termFrequency1\nUsuario2$Termino <- terminos2[,1]\nUsuario2$frecuencia <- termFrequency2\nUsuario3$Termino <- terminos3[,1]\nUsuario3$frecuencia <- termFrequency3\nUsuario4$Termino <- terminos4[,1]\nUsuario4$frecuencia <- termFrequency4\nUsuario5$Termino <- terminos5[,1]\nUsuario5$frecuencia <- termFrequency5\n\nnames(Usuario1)[2] <- paste(\"Usuario1\")\nnames(Usuario2)[2] <- paste(\"Usuario2\")\nnames(Usuario3)[2] <- paste(\"Usuario3\")\nnames(Usuario4)[2] <- paste(\"Usuario4\")\nnames(Usuario5)[2] <- paste(\"Usuario5\")\nrownames(Usuario1) <- Usuario1$Termino\nrownames(Usuario2) <- Usuario2$Termino\nrownames(Usuario3) <- Usuario3$Termino\nrownames(Usuario4) <- Usuario4$Termino\nrownames(Usuario5) <- Usuario5$Termino\nUsuario1$Termino <- NULL\nUsuario2$Termino <- NULL\nUsuario3$Termino <- NULL\nUsuario4$Termino <- NULL\nUsuario5$Termino <- NULL\n\n\n#Buscamos las relaciones entre los usuarios y obtemos subconjuntos de los data frames\n#Relacion del Usuario 1 con los demas\nRelacion12 <- merge(Usuario1, Usuario2, by = \"row.names\")\nRelacion13 <- merge(Usuario1, Usuario3, by = \"row.names\")\nRelacion14 <- merge(Usuario1, Usuario4, by = \"row.names\")\nRelacion15 <- merge(Usuario1, Usuario5, by = \"row.names\")\n\n#Relacion del usuario2 con los demas\nRelacion21 <- merge(Usuario2, Usuario1, by = \"row.names\")\nRelacion23 <- merge(Usuario2, Usuario3, by = \"row.names\")\nRelacion24 <- merge(Usuario2, Usuario4, by = \"row.names\")\nRelacion25 <- merge(Usuario2, Usuario5, by = \"row.names\")\n\n#Relacion del usuario 3 con los demas\nRelacion31 <- merge(Usuario3, Usuario1, by = \"row.names\")\nRelacion32 <- merge(Usuario3, Usuario2, by = \"row.names\")\nRelacion34 <- merge(Usuario3, Usuario4, by = \"row.names\")\nRelacion35 <- merge(Usuario3, Usuario5, by = \"row.names\")\n\n#Relacion del usuario 4 con el 5\nRelacion45 <- merge(Usuario4, Usuario5, by = \"row.names\")\n\n#Relacion del usuario 1 contra todos los demas\nRelacion1vsT <- merge(Relacion12, Relacion13, all = TRUE)\nRelacion1vsT <- merge(Relacion1vsT, Relacion14, all = TRUE)\nRelacion1vsT <- merge(Relacion1vsT, Relacion15, all = TRUE)\n\n#Relacion del usuario 2 contra todos los demas\nRelacion2vsT <- merge(Relacion21, Relacion23, all = TRUE)\nRelacion2vsT <- merge(Relacion2vsT, Relacion24, all = TRUE)\nRelacion2vsT <- merge(Relacion2vsT, Relacion25, all = TRUE)\n\n#Relacion del usuario 3 contra todos los demas\nRelacion3vsT <- merge(Relacion31, Relacion32, all = TRUE)\nRelacion3vsT <- merge(Relacion3vsT, Relacion34, all = TRUE)\nRelacion3vsT <- merge(Relacion3vsT, Relacion35, all = TRUE)\n\n#Relacion del usuario 4 contra todos los demas\nRelacion4vsT <- merge(Relacion14,Relacion24, all = TRUE)\nRelacion4vsT <- merge(Relacion4vsT, Relacion34, all = TRUE)\nRelacion4vsT <- merge(Relacion4vsT, Relacion45, all = TRUE)\n\n#Relacion del usuario 5 contra todos los demas\nRelacion5vsT <- merge(Relacion15, Relacion25, all = TRUE)\nRelacion5vsT <- merge(Relacion5vsT, Relacion35, all = TRUE)\nRelacion5vsT <- merge(Relacion5vsT, Relacion45, all = TRUE)\n\n#Relacion del de todos contra todos\n#En este Data Frame podemos observar las palabras en las que se relacionan unos con otros\nRelacion <- merge(Relacion1vsT, Relacion2vsT, all = TRUE)\nRelacion <- merge(Relacion, Relacion3vsT, all = TRUE)\nRelacion <- merge(Relacion, Relacion4vsT, all = TRUE)\nRelacion <- merge(Relacion, Relacion5vsT, all = TRUE)\nRelacion[is.na(Relacion)] <- 0\n\n\n```\n\n3. Aplique algun metodo de clasificacion para agrupar a los usuarios segun las palabras\nque elija.(justifque en el informe).\n\nEn este punto escogeremos ciertas palabras para clasificar los Usuarios segun lo que escriben en sus publicaciones\n```{r}\ncorpus <- Corpus(VectorSource(data$post))\ntdm <- TermDocumentMatrix(corpus,control=list(wordLengths=c(2,Inf)))\nTerminos<- (unlist(findFreqTerms(tdm, lowfreq=1)))\n\nallTerms = data.frame( Termino = character(10382), Frecuencia = numeric(10382))\nallTerms$Termino <- (unlist(findFreqTerms(tdm, lowfreq=1)))\nfrecuenciaTotal <- rowSums(as.matrix(tdm))\nallTerms$Frecuencia <- frecuenciaTotal\n\nterminosFrecuentes <- allTerms[ allTerms$Frecuencia > 50, ]\nrownames(terminosFrecuentes) <- terminosFrecuentes$Termino\nterminosFrecuentes$Termino <- NULL\ndim(terminosFrecuentes)\ndistMatrix <- dist(terminosFrecuentes)\nfit <- hclust(distMatrix, method=\"ward.D\")\nplot(fit)\nrect.hclust(fit, k=3)\n\n#Formaremos 3 subgrupos de palabras frecuentes\ngrupos <- cutree(fit, k = 3)\nterminosFrecuentes$Grupo <- grupos\n\naportes <- merge(terminosFrecuentes, Usuario1, all.x = TRUE, by = \"row.names\")\naportes <- merge(aportes, Usuario2, all.x = TRUE, by.x = \"Row.names\", by.y = \"row.names\")\naportes <- merge(aportes, Usuario3, all.x = TRUE, by.x = \"Row.names\", by.y = \"row.names\")\naportes <- merge(aportes, Usuario4, all.x = TRUE, by.x = \"Row.names\", by.y = \"row.names\")\naportes <- merge(aportes, Usuario5, all.x = TRUE, by.x = \"Row.names\", by.y = \"row.names\")\naportes[is.na(aportes)]<- 0\n\nfreqTerms <- findFreqTerms(tdm, lowfreq=50)\nmatrixClust <- aportes[c(\"Row.names\",\"Usuario1\", \"Usuario2\", \"Usuario3\", \"Usuario4\", \"Usuario5\")]\nmatrixClust <- t(matrixClust)\nmatrixClust <- matrixClust[2:6,]\n\ndistMatrix2 <- dist(matrixClust)\nfit2 <- hclust(distMatrix2, method = \"ward.D\")\nplot(fit2)\nrect.hclust(fit2, k=3)\n```\n\nEn el primer grafico podemos observar las palabras mas frecuentes, procederemos ahora a asignar clusters.\n\nEn el segundo podemos observar los clusters formados que por los usuarios segun su frecuencia aportada a las palabras mas frecuentes\n\nFormamos 3 grupos caracterizamos las palabras por su frecuencia, el grupo 1 esta conformado por palabras que tienen una frecuencia mayor que 50 pero menor que 100, el grupo 2 mayor que 100 y menor que 222, el grupo 3 por elementos con frecuencia mayor que 222 y menor que 414. \n\nAhora clasificaremos a los usuarios por su aporte a la frecuencia de dichos terminos y observaremos las relaciones a traves de un analisis de componentes principales. La relacion esta dada por la frecuencia aportada por los usuarios a la frecuencia total de las palabras mas comunes en el total de publicaciones de la red social Facebook luego de limpiar la data.\n\n```{r}\n#Observaremos relaciones entre los usuarios\naportes\n\n#Matriz de correlaciones\ncor(aportes[,4:8])\n\n#Analisis de componentes principales\ncomponentes <- PCA(aportes[, 4:8])\n\n```\n\nEn el Data Frame anterior podemos observar la frecuencia de los terminos mas comunes (frecuencia > 50) y el aporte de a dicha frecuencia por parte de cada usuario.\n\nEn el circulo de correlaciones se puede evidenciar lo antes observado en la clasificacion jerarquica, los usuarios 1 y 2 son medianamente inversamente proporcionales y ademas, no guardan casi relacion con los demas Usuarios. Es importante destacar, que segun el circulo de correlaciones, los individuos estan bastante bien representados en el plano formado por las 2 primeras componentes principales.\n\n```{r}\nGrupo1 <- aportes[aportes$Grupo==1,]\nGrupo2 <- aportes[aportes$Grupo==2,]\nGrupo3 <- aportes[aportes$Grupo==3,]\n\nGrupo1\nGrupo2\nGrupo3\n\nsummary(Grupo1)\nsummary(Grupo2)\nsummary(Grupo3)\n```\n\nEn los datos anteriores se puede observar los usuarios que mas aportan a una palabra en un determinado grupo, por ejemplo el Usuario2 aporta 90 de la frecuencia total de una palabra, el Usuario 5 en el grupo 3 aporta 104 y el Usuario3 en el grupo 1 aporta 76 a una palabra. Dichas palabras de pueden observar en los Data Frame antes mostrados con los elementos de los grupos\n\n---\n4. Genere un nuevo data frame en el que almacene una matriz individuos* individuos y\ncada entrada son las palabras que los relacionan, luego guardelo en un nuevo archivo\ncon el nombre CI1 CI2 facebook usuarios grupos.csv\n```{r}\nwrite.csv(Relacion, \"21014872_22022441_23194702_facebook_usuarios_grupos.csv\")\n```\n5. Define las relaciones que considere mas importantes.\n\nEn este punto es importante destacar que que existen usuarios que aportan casi toda la frecuencia total a ciertas palabras, en funcion a esto, podemos llegar a la conclusion que estos usuarios se diferencian de los demas, debido a que en sus Facebooks, hablan mucho mas de topicos relacionados con dicha palabra que los otros usuarios.\n\n```{r}\naportes\n```\n\nSe puede observar como por ejemplo el usuario 3 abarca gran parte de la frecuencia de las palabras \"dios\", \"papa\", \"francisco\". Por lo cual podemos inferir que el Usuario3 esta relacionado con la relacion Cristiana, siendo su maximo representante en la tierra (que tambien es un termino en el cual aporta gran parte de la frecuencia total) el papa Francisco.\n\nSe puede observar ademas, que los usuario 1,2 y 5 son los que mas aportan al grupo 3 del cluster de palabras, formado por las palabras \"haha\" de frecuencia 414 y la palabra video de frecuencia 326.\n\nUna relacion importante, es que todos los usuarios estan fuertementes relacionados por las palabras haha y video. En base a esto, se puede concluir que los amigos de los diferentes usuarios, tienden a \"escribir\" su risa y publicar videos (o hablar de ellos) de manera regular en la red social facebook.\n\nEn base a los resultados de la clusterizacion tanto de palabras como de individuos, queda evidencia que los Usuarios 1 y 2 pertenecen a grupos diferentes donde hablan de temas muy diferentes entre si, y que casi no guardan relacion con los demas usuarios.Ademas, los Usuarios 1,4 y 5, en funcion a la informacion obtenida a traves de la frecuencia de las palabras comunes, se evidencia que hablan de temas muy parecidos entre si.",
    "created" : 1439389724407.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1106023390",
    "id" : "EAB78B0B",
    "lastKnownWriteTime" : 1439397395,
    "path" : "~/Data Science/ProyectoICD/informe.Rmd",
    "project_path" : "informe.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}